# -*- coding: utf-8 -*-
"""text_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LkPKdHX_is3iFrv-xMUeRd8xRoCMb-l6
"""

import gdown
import pandas as pd
import numpy as np
import re
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split

url = "https://drive.google.com/uc?id=1wCr0baMLVyx4MtEKGq3895ZFO8IqXmGt"
output = "tononkira.csv"
gdown.download(url,output)

"""# Preprocessing"""

data = pd.read_csv("tononkira.csv")
data = data.drop("Unnamed: 0",axis=1)

data["Label"] = data["Label"].replace({"bodo":1,"poopy":0})

def buildDict(texts):
  hashmap = {}

  for text in texts:
    for word in text.split():
      hashmap[word] = hashmap.get(word,0) + 1

  return {w:i+1 for i, w in enumerate(list(hashmap.keys()))}

def calculateMaxSeqLen(texts):
    """
    Calculates the maximum sequence length found in the corpus
    """
    max_len = float('-inf')
    for text in texts:
        if len(text.split()) > max_len:
            max_len = len(text.split())

    return max_len

dictionary = buildDict(data["text"])
max_seq_len = calculateMaxSeqLen(data["text"])

# print(f"Number of words in dictionary: {len(dictionary)}")
# print(f"Maximum sequence lenght: {max_seq_len}")

"""# Data loader"""

BATCH_SIZE = 16
LSTM_LAYERS = 1
LEARNING_RATE = 0.001
DROPOUT = 0.1
INPUT_SIZE = len(dictionary) + 1
HIDDEN_DIM = 64
EPOCHS = 20
MAX_SEQ_LEN = max_seq_len

class DataHandler(Dataset):
    """
    Iterator generator for data loader construction
    """

    @staticmethod
    def tokenizer(X, max_seq_len, dicitionary):
        """
        Given a sequence of words, tokenize each word with
        use of `dictionary` and apply padding considering `max_seq_len`
        """
        sequences = []
        for x in X:
            sequence = [0] * max_seq_len
            for idx, word in enumerate(x.split()):
                sequence[idx] = dictionary[word]
            sequences.append(sequence)

        return np.array(sequences)

    def __init__(self, x, y, max_seq_len, dictionary):
        self.x = self.tokenizer(x, max_seq_len, dictionary)
        self.y = y.to_numpy()

    def __len__(self):
        return len(self.x)

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

#Split data into training and testing
x_train, x_test, y_train, y_test = train_test_split(data["text"], data["Label"], test_size=0.3, stratify=data["Label"])

##Initialize iterator objects for the data loader
train = DataHandler(x_train, y_train, max_seq_len, dictionary)
test = DataHandler(x_test, y_test, max_seq_len, dictionary)

#Initialize data loaders
loader_training = DataLoader(train, batch_size=BATCH_SIZE, shuffle=False)
loader_test = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)

"""# Model"""

#Identify device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class TextClassifier(nn.ModuleList):
    """
    LSTM Network definition
    """
    def __init__(self):
        super(TextClassifier, self).__init__()

        self.batch_size = BATCH_SIZE
        self.hidden_dim = HIDDEN_DIM
        self.LSTM_layers = LSTM_LAYERS
        self.input_size = INPUT_SIZE

        self.leakyRelu = nn.LeakyReLU(0.3)

        #self.dropout = nn.Dropout(DROPOUT)
        self.embedding = nn.Embedding(num_embeddings=self.input_size, embedding_dim=self.hidden_dim, padding_idx=0)
        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers, batch_first=True, bidirectional=True)

        # Since we will be implementing a Bi-LSTM network, the expected vector
        # size is doubled. For this reason we multiply x 2.
        self.fc1 = nn.Linear(in_features=self.hidden_dim * 2, out_features=64)
        self.fc2 = nn.Linear(64, 1)

    def forward(self, x):
        #The memory and state tensors of the LSTM are also multiplied x2 to address bilateral behavior.
        h = torch.zeros((self.LSTM_layers * 2, x.size(0), self.hidden_dim)).to(device)
        c = torch.zeros((self.LSTM_layers * 2, x.size(0), self.hidden_dim)).to(device)

        torch.nn.init.xavier_normal_(h)
        torch.nn.init.xavier_normal_(c)

        out = self.embedding(x)
        out, (hidden, cell) = self.lstm(out, (h, c))
        #out = self.dropout(out)
        out = self.leakyRelu(self.fc1(out[:,-1,:]))
        #out = self.dropout(out)
        out = torch.sigmoid(self.fc2(out))

        return out.squeeze()

# Model initialization
model = TextClassifier().to(device)

# Optimizer initialization
#optimizer = optim.Adagrad(model.parameters(), lr=LEARNING_RATE)
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.2)

def calculate_accuray(grand_truth, predictions):
    """
    Accuracy calcuation: (tp + tn) / N
    """
    true_positives, true_negatives = 0, 0
    for true, pred in zip(grand_truth, predictions):
        if (pred >= 0.5) and (true == 1):
            true_positives += 1
        elif (pred < 0.5) and (true == 0):
            true_negatives += 1

    return (true_positives+true_negatives) / len(grand_truth)

"""# Training and evaluation"""

# Training
for epoch in range(EPOCHS):
    model.train()
    train_predictions = []
    for x_batch, y_batch in loader_training:

        x = x_batch.type(torch.LongTensor).to(device)
        y = y_batch.type(torch.FloatTensor).to(device)

        y_pred = model(x)

        loss = F.binary_cross_entropy(y_pred, y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Save predictions for metrics calculation
        train_predictions += list(y_pred.squeeze().cpu().detach().numpy())

    # Evaluation
    with torch.no_grad():
        model.eval()
        test_predictions = []
        for x_batch, y_batch in loader_test:
           x = x_batch.type(torch.LongTensor).to(device)
           y = y_batch.type(torch.FloatTensor).to(device)

           y_pred = model(x)
           # Save predictions for metrics calculation
           test_predictions += list(y_pred.squeeze().cpu().detach().numpy())

    train_accuracy = calculate_accuray(y_train, train_predictions)
    test_accuracy = calculate_accuray(y_test, test_predictions)
    print(f"epoch: {epoch+1}, loss: {loss.item():.5f}, train: {train_accuracy:.5f}, test: {test_accuracy:.5f}")